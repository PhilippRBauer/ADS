{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2 : Data Modeling\n",
    "Based on the EDA from Task 1, we used the knowledge gained by our findings in the Exploration task to model our baseline model with minimal preprocessing using the wine dataset, provided in the first task. Afterwards, based on the scores we gathered, we preprocessed the data set by identifying the data quality issues and handling them.\n",
    "- Feature und Instance Selection?\n",
    "We then went on to define different pipelines with three different dataframes where we examined three different analysis approaches (univariate, multivariate and a combination of both) considering outliers and noise.\n",
    "\n",
    "## To decide and evaluating our different model options:\n",
    "  - Implementation of GridSearchCV, which should consider:\n",
    "    - Function to log our results in a JSON file, where:\n",
    "        - preprocesssing steps were conducted\n",
    "        - Hyper-Parameters of the preprocessing methods/ algorithms are included\n",
    "        - Our algorithm that is to be applied is modeled\n",
    "        - and the Hyper-Parameters of the modeling algorithm\n",
    "    - Visualization of our results\n",
    "\n",
    "\n",
    "In our case we first started experimenting with classification, but we were not able to get over 62% and the split data set to help us predict which type of wine we have was not functioning the way we wanted.\n",
    "We thought about the characteristics of the data set again and decided that it was more plausible to explain which input variables are the most significant for the influence of the output variable.\n",
    "\n",
    "Also, here we need to take into account that our dataset can be quantified or measured numerically, and therefore it has quantitative features/ continuous variables.\n",
    "\n",
    "Regression models are  well-suited for predicting continuous variables, whereas classification models are better suited for predicting categorical variables. Additionally, having a large number of observations (4000 rows) and a small number of features (16+1 label) makes it a good choice as it allows for a sufficient amount of data to train the model while still allowing for easy interpretability and understand of the model's predictions. Additionally, the fact that the data is numerical in nature, with a small number of outliers and missing values, makes it a good candidate for regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Content\n",
    "\n",
    "1. [Import and Data Loading](#import-and-data-loading),\n",
    "2. [Standard Pre-Processing](#standard-pre-processing),\n",
    "    2.1 [MissingValues](#missing-values),\n",
    "3. [Baseline Regression Models](#baseline-regression-models),\n",
    "    3.1 [Classification Models](#classification-models),\n",
    "    3.2 [Regression Models](#regression-models),\n",
    "4. [Preprocessing](#preprocessing),\n",
    "    4.1 [Separate](#conclusion-missing-values),\n",
    "5. [Definition Pipelines](#definition-pipelines),\n",
    "6. [Reading JSON preprocessing parameters, models and preprocessing](#reading-json-preprocessing-parameters-models-and-preprocessing),\n",
    "    6.1 [Outliers without Gamay](#outliers-without-gamay),\n",
    "7. [Best R^2 and MSE: RandomForestRegressor](#best-r2-and-mse:-randomforestregressor)\n",
    "8. [RandomizedSearchCV](#randomizedsearchcv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functions import missing_value_handling, remove_multivariate_outlier, remove_univariate_outliers, uni_multi_variate_outlier, normalize, standardize, noise_filtering\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from typing import Dict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from onedal.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "date_time = datetime.now().strftime(\"%d.%m.%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading data and standard preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read CSV file to load data\n",
    "df_wine = pd.read_csv('wine_training.csv', sep=';', index_col='Unnamed: 0')\n",
    "\n",
    "# Changing column names -> space to underscore\n",
    "df_wine.columns = df_wine.columns.str.replace(' ', '_')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standard Preprocessing\n",
    "\n",
    "- Missing Value Handling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "df_wine = missing_value_handling(df_wine)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Regression Models\n",
    "\n",
    "For diffeerent regressions and classification\n",
    "\n",
    "Split Train Test:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "X = df_wine.drop(['wine_type', 'quality'], axis = 1)\n",
    "y = df_wine['quality']\n",
    "\n",
    "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X, y, test_size=.25, random_state=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abandoned approach: Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline:\n",
      "   CrossValMeans  CrossValerrors               Algorithm\n",
      "0       0.434774        0.018629           LogRegression\n",
      "1       0.608928        0.019456  RandomForestClassifier\n",
      "2       0.623605        0.020252    ExtraTreesClassifier\n",
      "3       0.437438        0.001075                     SVC\n",
      "4       0.456781        0.030462              GaussianNB\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from onedal.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "classification_models = [LogisticRegression(),RandomForestClassifier(),ExtraTreesClassifier(),SVC(C=1, kernel='rbf'),GaussianNB()]\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Modeling step Test differents algorithms\n",
    "\n",
    "\n",
    "results = []\n",
    "for model in classification_models:\n",
    "    results.append(cross_val_score(model, X_train_base, y = y_train_base, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "\n",
    "for cv_result in results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":['LogRegression','RandomForestClassifier','ExtraTreesClassifier','SVC','GaussianNB']})\n",
    "print(\"Pipeline:\")\n",
    "\n",
    "print(cv_res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pursued approach: Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: LinearRegression()\n",
      "Mean squared error: 113.88239797806641\n",
      "R-squared: -0.00481638430081599\n",
      "Regularization parameter: LinearSVR(random_state=42)\n",
      "Mean squared error: 113.36805323569745\n",
      "R-squared: -0.0002781761713135378\n",
      "Regularization parameter: KNeighborsRegressor()\n",
      "Mean squared error: 127.88904904904905\n",
      "R-squared: -0.12840100084549566\n",
      "Regularization parameter: RandomForestRegressor(random_state=42)\n",
      "Mean squared error: 122.76703273273272\n",
      "R-squared: -0.08320801223032803\n",
      "Regularization parameter: GradientBoostingRegressor(random_state=42)\n",
      "Mean squared error: 119.88500738123382\n",
      "R-squared: -0.05777909305956941\n",
      "Regularization parameter: DecisionTreeRegressor(random_state=42)\n",
      "Mean squared error: 264.07107107107106\n",
      "R-squared: -1.3299732315364676\n"
     ]
    }
   ],
   "source": [
    "regression_models = [LinearRegression(),LinearSVR(random_state = 42),KNeighborsRegressor(),RandomForestRegressor(random_state = 42), GradientBoostingRegressor(random_state = 42),DecisionTreeRegressor(random_state = 42)]\n",
    "\n",
    "def baseline_regression_model(reg_models, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Create an empty list to store the models\n",
    "    models = []\n",
    "\n",
    "    for reg in reg_models:\n",
    "        reg.fit(X_train, y_train)\n",
    "        # Append the model to the list\n",
    "        models.append(reg)\n",
    "        # Predict on the training data\n",
    "        y_pred = reg.predict(X_test)\n",
    "        #print(len(y_pred))\n",
    "        #print(len(y_pred))\n",
    "        # Calculate the mean squared error and R-squared\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        # Print the results\n",
    "        print(\"Regularization parameter:\", reg)\n",
    "        print(\"Mean squared error:\", mse)\n",
    "        print(\"R-squared:\", r2)\n",
    "\n",
    "baseline_regression_model(regression_models, X_train_base, X_test_base, y_train_base, y_test_base)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further preprocessing\n",
    "\n",
    "Missing Values are already handled\n",
    "\n",
    "1. Remove 99 quality values (skewing predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "               wine_type  fixed_acidity  volatile_acidity  citric_acid  \\\n0             Pinot noir            5.8              0.15         0.49   \n1                 Merlot            6.6              0.25         0.32   \n2             Chardonnay            6.7              0.21         0.34   \n3                 Merlot            8.3              0.28         0.27   \n4                 Merlot            7.5              0.42         0.19   \n...                  ...            ...               ...          ...   \n3995  Cabernet Sauvignon            6.4              0.23         0.37   \n3996  Cabernet Sauvignon            7.0              0.22         0.26   \n3997              Merlot            7.5              0.26         0.30   \n3998          Chardonnay            6.3              0.43         0.32   \n3999              Merlot            6.6              0.37         0.07   \n\n      residual_sugar  magnesium  flavanoids    minerals  calcium  chlorides  \\\n0                1.1  76.729301      894.94  186.639301   109.91      0.048   \n1                5.6   4.795712     1160.95  251.875712   247.08      0.039   \n2                1.5  85.193710      789.82  304.703710   219.51      0.035   \n3               17.5  11.976525      777.86  237.586525   225.61      0.045   \n4                6.9   5.599673      785.72   95.399673    89.80      0.041   \n...              ...        ...         ...         ...      ...        ...   \n3995             7.9  92.701914     1143.32  318.791914   226.09      0.050   \n3996             9.2  94.807955      863.32  322.107955   227.30      0.027   \n3997             4.6  50.112474      831.67  360.872474   310.76      0.027   \n3998             8.8  84.805688     1300.32  328.915688   244.11      0.042   \n3999             1.4  28.038626     1190.75  129.198626   101.16      0.048   \n\n      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n0                    21.0                  98.0  0.99290  3.19       0.48   \n1                    15.0                  68.0  0.99163  2.96       0.52   \n2                    45.0                 123.0  0.98949  3.24       0.36   \n3                    48.0                 253.0  1.00014  3.02       0.56   \n4                    62.0                 150.0  0.99508  3.23       0.37   \n...                   ...                   ...      ...   ...        ...   \n3995                 60.0                 150.0  0.99488  2.86       0.49   \n3996                 37.0                 122.0  0.99228  3.06       0.34   \n3997                 29.0                  92.0  0.99085  3.15       0.38   \n3998                 18.0                 106.0  0.99172  3.28       0.33   \n3999                 58.0                 144.0  0.99220  3.17       0.38   \n\n      alcohol  quality  \n0         9.2        5  \n1        11.1        6  \n2        12.6        7  \n3         9.1        6  \n4        10.0        6  \n...       ...      ...  \n3995      9.3        6  \n3996     12.5        8  \n3997     12.0        7  \n3998     12.9        7  \n3999     10.0        5  \n\n[3961 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wine_type</th>\n      <th>fixed_acidity</th>\n      <th>volatile_acidity</th>\n      <th>citric_acid</th>\n      <th>residual_sugar</th>\n      <th>magnesium</th>\n      <th>flavanoids</th>\n      <th>minerals</th>\n      <th>calcium</th>\n      <th>chlorides</th>\n      <th>free_sulfur_dioxide</th>\n      <th>total_sulfur_dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pinot noir</td>\n      <td>5.8</td>\n      <td>0.15</td>\n      <td>0.49</td>\n      <td>1.1</td>\n      <td>76.729301</td>\n      <td>894.94</td>\n      <td>186.639301</td>\n      <td>109.91</td>\n      <td>0.048</td>\n      <td>21.0</td>\n      <td>98.0</td>\n      <td>0.99290</td>\n      <td>3.19</td>\n      <td>0.48</td>\n      <td>9.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Merlot</td>\n      <td>6.6</td>\n      <td>0.25</td>\n      <td>0.32</td>\n      <td>5.6</td>\n      <td>4.795712</td>\n      <td>1160.95</td>\n      <td>251.875712</td>\n      <td>247.08</td>\n      <td>0.039</td>\n      <td>15.0</td>\n      <td>68.0</td>\n      <td>0.99163</td>\n      <td>2.96</td>\n      <td>0.52</td>\n      <td>11.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chardonnay</td>\n      <td>6.7</td>\n      <td>0.21</td>\n      <td>0.34</td>\n      <td>1.5</td>\n      <td>85.193710</td>\n      <td>789.82</td>\n      <td>304.703710</td>\n      <td>219.51</td>\n      <td>0.035</td>\n      <td>45.0</td>\n      <td>123.0</td>\n      <td>0.98949</td>\n      <td>3.24</td>\n      <td>0.36</td>\n      <td>12.6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Merlot</td>\n      <td>8.3</td>\n      <td>0.28</td>\n      <td>0.27</td>\n      <td>17.5</td>\n      <td>11.976525</td>\n      <td>777.86</td>\n      <td>237.586525</td>\n      <td>225.61</td>\n      <td>0.045</td>\n      <td>48.0</td>\n      <td>253.0</td>\n      <td>1.00014</td>\n      <td>3.02</td>\n      <td>0.56</td>\n      <td>9.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Merlot</td>\n      <td>7.5</td>\n      <td>0.42</td>\n      <td>0.19</td>\n      <td>6.9</td>\n      <td>5.599673</td>\n      <td>785.72</td>\n      <td>95.399673</td>\n      <td>89.80</td>\n      <td>0.041</td>\n      <td>62.0</td>\n      <td>150.0</td>\n      <td>0.99508</td>\n      <td>3.23</td>\n      <td>0.37</td>\n      <td>10.0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>Cabernet Sauvignon</td>\n      <td>6.4</td>\n      <td>0.23</td>\n      <td>0.37</td>\n      <td>7.9</td>\n      <td>92.701914</td>\n      <td>1143.32</td>\n      <td>318.791914</td>\n      <td>226.09</td>\n      <td>0.050</td>\n      <td>60.0</td>\n      <td>150.0</td>\n      <td>0.99488</td>\n      <td>2.86</td>\n      <td>0.49</td>\n      <td>9.3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>Cabernet Sauvignon</td>\n      <td>7.0</td>\n      <td>0.22</td>\n      <td>0.26</td>\n      <td>9.2</td>\n      <td>94.807955</td>\n      <td>863.32</td>\n      <td>322.107955</td>\n      <td>227.30</td>\n      <td>0.027</td>\n      <td>37.0</td>\n      <td>122.0</td>\n      <td>0.99228</td>\n      <td>3.06</td>\n      <td>0.34</td>\n      <td>12.5</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>Merlot</td>\n      <td>7.5</td>\n      <td>0.26</td>\n      <td>0.30</td>\n      <td>4.6</td>\n      <td>50.112474</td>\n      <td>831.67</td>\n      <td>360.872474</td>\n      <td>310.76</td>\n      <td>0.027</td>\n      <td>29.0</td>\n      <td>92.0</td>\n      <td>0.99085</td>\n      <td>3.15</td>\n      <td>0.38</td>\n      <td>12.0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>Chardonnay</td>\n      <td>6.3</td>\n      <td>0.43</td>\n      <td>0.32</td>\n      <td>8.8</td>\n      <td>84.805688</td>\n      <td>1300.32</td>\n      <td>328.915688</td>\n      <td>244.11</td>\n      <td>0.042</td>\n      <td>18.0</td>\n      <td>106.0</td>\n      <td>0.99172</td>\n      <td>3.28</td>\n      <td>0.33</td>\n      <td>12.9</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>Merlot</td>\n      <td>6.6</td>\n      <td>0.37</td>\n      <td>0.07</td>\n      <td>1.4</td>\n      <td>28.038626</td>\n      <td>1190.75</td>\n      <td>129.198626</td>\n      <td>101.16</td>\n      <td>0.048</td>\n      <td>58.0</td>\n      <td>144.0</td>\n      <td>0.99220</td>\n      <td>3.17</td>\n      <td>0.38</td>\n      <td>10.0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>3961 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine_clean = df_wine[df_wine.quality != 99]\n",
    "df_wine_clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Separate Gamay and No-Gamay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "df_wine_no_gamay = df_wine_clean[df_wine_clean[\"wine_type\"]!=\"Gamay\"].copy()\n",
    "df_wine_gamay = df_wine_clean[df_wine_clean[\"wine_type\"]==\"Gamay\"].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. OneHotEncode Wine Type for df_wine_clean, df_wine_no_gamay\n",
    "4. Drop Wine Type for df_wine_gamay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  magnesium  \\\n0               5.8              0.15         0.49             1.1  76.729301   \n1               6.6              0.25         0.32             5.6   4.795712   \n2               6.7              0.21         0.34             1.5  85.193710   \n3               8.3              0.28         0.27            17.5  11.976525   \n4               7.5              0.42         0.19             6.9   5.599673   \n...             ...               ...          ...             ...        ...   \n3995            6.4              0.23         0.37             7.9  92.701914   \n3996            7.0              0.22         0.26             9.2  94.807955   \n3997            7.5              0.26         0.30             4.6  50.112474   \n3998            6.3              0.43         0.32             8.8  84.805688   \n3999            6.6              0.37         0.07             1.4  28.038626   \n\n      flavanoids    minerals  calcium  chlorides  free_sulfur_dioxide  ...  \\\n0         894.94  186.639301   109.91      0.048                 21.0  ...   \n1        1160.95  251.875712   247.08      0.039                 15.0  ...   \n2         789.82  304.703710   219.51      0.035                 45.0  ...   \n3         777.86  237.586525   225.61      0.045                 48.0  ...   \n4         785.72   95.399673    89.80      0.041                 62.0  ...   \n...          ...         ...      ...        ...                  ...  ...   \n3995     1143.32  318.791914   226.09      0.050                 60.0  ...   \n3996      863.32  322.107955   227.30      0.027                 37.0  ...   \n3997      831.67  360.872474   310.76      0.027                 29.0  ...   \n3998     1300.32  328.915688   244.11      0.042                 18.0  ...   \n3999     1190.75  129.198626   101.16      0.048                 58.0  ...   \n\n      density    pH  sulphates  alcohol  quality  Cabernet Sauvignon  \\\n0     0.99290  3.19       0.48      9.2        5                 0.0   \n1     0.99163  2.96       0.52     11.1        6                 0.0   \n2     0.98949  3.24       0.36     12.6        7                 0.0   \n3     1.00014  3.02       0.56      9.1        6                 0.0   \n4     0.99508  3.23       0.37     10.0        6                 0.0   \n...       ...   ...        ...      ...      ...                 ...   \n3995  0.99488  2.86       0.49      9.3        6                 1.0   \n3996  0.99228  3.06       0.34     12.5        8                 1.0   \n3997  0.99085  3.15       0.38     12.0        7                 0.0   \n3998  0.99172  3.28       0.33     12.9        7                 0.0   \n3999  0.99220  3.17       0.38     10.0        5                 0.0   \n\n      Chardonnay  Gamay  Merlot  Pinot noir  \n0            0.0    0.0     0.0         1.0  \n1            0.0    0.0     1.0         0.0  \n2            1.0    0.0     0.0         0.0  \n3            0.0    0.0     1.0         0.0  \n4            0.0    0.0     1.0         0.0  \n...          ...    ...     ...         ...  \n3995         0.0    0.0     0.0         0.0  \n3996         0.0    0.0     0.0         0.0  \n3997         0.0    0.0     1.0         0.0  \n3998         1.0    0.0     0.0         0.0  \n3999         0.0    0.0     1.0         0.0  \n\n[3961 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed_acidity</th>\n      <th>volatile_acidity</th>\n      <th>citric_acid</th>\n      <th>residual_sugar</th>\n      <th>magnesium</th>\n      <th>flavanoids</th>\n      <th>minerals</th>\n      <th>calcium</th>\n      <th>chlorides</th>\n      <th>free_sulfur_dioxide</th>\n      <th>...</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Cabernet Sauvignon</th>\n      <th>Chardonnay</th>\n      <th>Gamay</th>\n      <th>Merlot</th>\n      <th>Pinot noir</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.8</td>\n      <td>0.15</td>\n      <td>0.49</td>\n      <td>1.1</td>\n      <td>76.729301</td>\n      <td>894.94</td>\n      <td>186.639301</td>\n      <td>109.91</td>\n      <td>0.048</td>\n      <td>21.0</td>\n      <td>...</td>\n      <td>0.99290</td>\n      <td>3.19</td>\n      <td>0.48</td>\n      <td>9.2</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.6</td>\n      <td>0.25</td>\n      <td>0.32</td>\n      <td>5.6</td>\n      <td>4.795712</td>\n      <td>1160.95</td>\n      <td>251.875712</td>\n      <td>247.08</td>\n      <td>0.039</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>0.99163</td>\n      <td>2.96</td>\n      <td>0.52</td>\n      <td>11.1</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6.7</td>\n      <td>0.21</td>\n      <td>0.34</td>\n      <td>1.5</td>\n      <td>85.193710</td>\n      <td>789.82</td>\n      <td>304.703710</td>\n      <td>219.51</td>\n      <td>0.035</td>\n      <td>45.0</td>\n      <td>...</td>\n      <td>0.98949</td>\n      <td>3.24</td>\n      <td>0.36</td>\n      <td>12.6</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.3</td>\n      <td>0.28</td>\n      <td>0.27</td>\n      <td>17.5</td>\n      <td>11.976525</td>\n      <td>777.86</td>\n      <td>237.586525</td>\n      <td>225.61</td>\n      <td>0.045</td>\n      <td>48.0</td>\n      <td>...</td>\n      <td>1.00014</td>\n      <td>3.02</td>\n      <td>0.56</td>\n      <td>9.1</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.5</td>\n      <td>0.42</td>\n      <td>0.19</td>\n      <td>6.9</td>\n      <td>5.599673</td>\n      <td>785.72</td>\n      <td>95.399673</td>\n      <td>89.80</td>\n      <td>0.041</td>\n      <td>62.0</td>\n      <td>...</td>\n      <td>0.99508</td>\n      <td>3.23</td>\n      <td>0.37</td>\n      <td>10.0</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>6.4</td>\n      <td>0.23</td>\n      <td>0.37</td>\n      <td>7.9</td>\n      <td>92.701914</td>\n      <td>1143.32</td>\n      <td>318.791914</td>\n      <td>226.09</td>\n      <td>0.050</td>\n      <td>60.0</td>\n      <td>...</td>\n      <td>0.99488</td>\n      <td>2.86</td>\n      <td>0.49</td>\n      <td>9.3</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>7.0</td>\n      <td>0.22</td>\n      <td>0.26</td>\n      <td>9.2</td>\n      <td>94.807955</td>\n      <td>863.32</td>\n      <td>322.107955</td>\n      <td>227.30</td>\n      <td>0.027</td>\n      <td>37.0</td>\n      <td>...</td>\n      <td>0.99228</td>\n      <td>3.06</td>\n      <td>0.34</td>\n      <td>12.5</td>\n      <td>8</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>7.5</td>\n      <td>0.26</td>\n      <td>0.30</td>\n      <td>4.6</td>\n      <td>50.112474</td>\n      <td>831.67</td>\n      <td>360.872474</td>\n      <td>310.76</td>\n      <td>0.027</td>\n      <td>29.0</td>\n      <td>...</td>\n      <td>0.99085</td>\n      <td>3.15</td>\n      <td>0.38</td>\n      <td>12.0</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>6.3</td>\n      <td>0.43</td>\n      <td>0.32</td>\n      <td>8.8</td>\n      <td>84.805688</td>\n      <td>1300.32</td>\n      <td>328.915688</td>\n      <td>244.11</td>\n      <td>0.042</td>\n      <td>18.0</td>\n      <td>...</td>\n      <td>0.99172</td>\n      <td>3.28</td>\n      <td>0.33</td>\n      <td>12.9</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>6.6</td>\n      <td>0.37</td>\n      <td>0.07</td>\n      <td>1.4</td>\n      <td>28.038626</td>\n      <td>1190.75</td>\n      <td>129.198626</td>\n      <td>101.16</td>\n      <td>0.048</td>\n      <td>58.0</td>\n      <td>...</td>\n      <td>0.99220</td>\n      <td>3.17</td>\n      <td>0.38</td>\n      <td>10.0</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3961 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine_gamay = df_wine_gamay.copy().drop(columns=[\"wine_type\"])\n",
    "df_wine_gamay\n",
    "\n",
    "# One Hot Encoding\n",
    "wine_type_array = df_wine_no_gamay[\"wine_type\"].to_numpy()\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "wine_type_transformed = ohe.fit_transform(wine_type_array.reshape(-1, 1))  # 1d array -> 2d array\n",
    "df_wine_no_gamay[ohe.categories_[0]] = wine_type_transformed\n",
    "df_wine_no_gamay = df_wine_no_gamay.copy().drop(columns=[\"wine_type\"])\n",
    "#df_wine_no_gamay\n",
    "\n",
    "# OneHotEncoding df_clean\n",
    "wine_type_array = df_wine_clean[\"wine_type\"].to_numpy()\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "wine_type_transformed = ohe.fit_transform(wine_type_array.reshape(-1, 1))  # 1d array -> 2d array\n",
    "df_wine_clean[ohe.categories_[0]] = wine_type_transformed\n",
    "df_wine_clean = df_wine_clean.copy().drop(columns=[\"wine_type\"])\n",
    "df_wine_clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This results in three dataframes to be used for modelling:\n",
    "- df_wine_clean, containing all wines\n",
    "- df_wine_no_gamay\n",
    "- df_wine_gamay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining preprocessing pipelines\n",
    "\n",
    "The pipelines implement the following preprocessing steps:\n",
    "- `pipeline1`: Univariate outliers and noise filtering\n",
    "- `pipeline2`: Multivariate outliers and noise filtering\n",
    "- `pipeline3`: Univariate outliers, multivariate outliers and noise filtering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# univariate outlier + noise\n",
    "def pipeline1(df, input_df=\"\"):\n",
    "    temp_df=df.copy()\n",
    "    temp_df = remove_univariate_outliers(temp_df)\n",
    "    temp_df = noise_filtering(temp_df, 1)\n",
    "    X = temp_df.drop('quality', axis='columns')\n",
    "    y = temp_df['quality']\n",
    "    # split off some data for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    return {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'input_df': input_df,\n",
    "        'preprocessing_steps': 'univariate outlier + noise'\n",
    "    }\n",
    "pipeline1_clean = pipeline1(df_wine_clean, \"df_wine_clean\")\n",
    "pipeline1_no_gamay = pipeline1(df_wine_no_gamay, \"df_wine_no_gamay\")\n",
    "pipeline1_gamay = pipeline1(df_wine_gamay, \"df_wine_gamay\")\n",
    "\n",
    "# multivariate outlier + noise\n",
    "def pipeline2(df, input_df=\"\"):\n",
    "    temp_df=df.copy()\n",
    "    temp_df = remove_multivariate_outlier(temp_df).drop(columns=[\"outliers\"])\n",
    "    #print(temp_df)\n",
    "    temp_df = noise_filtering(temp_df, 1)\n",
    "    X = temp_df.drop('quality', axis='columns')\n",
    "    y = temp_df['quality']\n",
    "    # split off some data for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    return {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'input_df': input_df,\n",
    "        'preprocessing_steps': 'multivariate outlier + noise'\n",
    "    }\n",
    "pipeline2_clean = pipeline2(df_wine_clean, \"df_wine_clean\")\n",
    "pipeline2_no_gamay = pipeline2(df_wine_no_gamay, \"df_wine_no_gamay\")\n",
    "pipeline2_gamay = pipeline2(df_wine_gamay, \"df_wine_gamay\")\n",
    "\n",
    "# uni+multivariate outlier + noise\n",
    "def pipeline3(df, input_df=\"\"):\n",
    "    temp_df=df.copy()\n",
    "    temp_df = uni_multi_variate_outlier(temp_df).drop(columns=[\"outliers\"])\n",
    "    temp_df = noise_filtering(temp_df, 1)\n",
    "    X = temp_df.drop('quality', axis='columns')\n",
    "    y = temp_df['quality']\n",
    "    # split off some data for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    return {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'input_df': input_df,\n",
    "        'preprocessing_steps': 'uni-multivariate outlier + noise'\n",
    "    }\n",
    "pipeline3_clean = pipeline3(df_wine_clean, \"df_wine_clean\")\n",
    "pipeline3_no_gamay = pipeline3(df_wine_no_gamay, \"df_wine_no_gamay\")\n",
    "pipeline3_gamay = pipeline3(df_wine_gamay, \"df_wine_gamay\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predictions for the different dataframes are to be made using every pipeline:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "pipelines_clean = [pipeline1_clean,pipeline2_clean,pipeline3_clean]\n",
    "\n",
    "pipelines_no_gamay = [pipeline1_no_gamay, pipeline2_no_gamay, pipeline3_no_gamay]\n",
    "\n",
    "pipelines_gamay =[pipeline1_gamay, pipeline2_gamay, pipeline3_gamay]\n",
    "\n",
    "pipelines = [pipeline1_clean,pipeline2_clean,pipeline3_clean, pipeline1_no_gamay, pipeline2_no_gamay, pipeline3_no_gamay, pipeline1_gamay, pipeline2_gamay, pipeline3_gamay]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grid search\n",
    "\n",
    "## Logging\n",
    "\n",
    "The result and composition of each combination of model and preprocessing steps are being documented in `logging.log` following this style:\n",
    "\n",
    "``15.01.2023, 16:51:27: Input dataframe: df_wine_gamay, Preprocessing: univariate outlier + noise``<br>\n",
    "``15.01.2023, 16:51:27: Best Parameters for  KNeighborsRegressor(): {'model__algorithm': 'ball_tree', 'model__n_neighbors': 16, 'model__p': 3, 'model__weights': 'distance'}``<br>\n",
    "``15.01.2023, 16:51:27: Best Score for  KNeighborsRegressor(): 0.10587181662752698``<br>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Logging: Noch auslagern\n",
    "outputfile = 'logging.log'\n",
    "def init_logging(file_path, truncate):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        # Create the file\n",
    "        open(file_path, 'w').close()\n",
    "    else:\n",
    "        if truncate:\n",
    "            with open(file_path, 'w') as f:\n",
    "                # Truncate the file\n",
    "                f.truncate(0)\n",
    "init_logging(outputfile, truncate=0)\n",
    "\n",
    "\n",
    "def write_to_log_file(msg, file = 'logging.log'):\n",
    "    print(msg)\n",
    "    # Open the file in append mode\n",
    "    with open(file, 'a', newline='') as logfile:\n",
    "        logfile.write(f\"{date_time}: {msg}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applying preprocessing parameters, models and hyperparameters\n",
    "\n",
    "- Hyperparameters are loaded from `hyperparameters.json`\n",
    "- ```grid_search.best_score``` returns an R² value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pipeline in pipelines:\n",
    "    print(pipeline['X'].shape)\n",
    "\n",
    "with open('hyperparameters.json') as f:\n",
    "        hyperparameters = json.load(f)\n",
    "\n",
    "def grid_search(model, pipe, pipeline):\n",
    "    keys_to_remove=models[model]['keys_to_remove']\n",
    "    param_grid={key: value for key, value in models[model].items() if key not in keys_to_remove}\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "    grid_search.fit(pipeline['X'], pipeline['y'])\n",
    "    # Print the best parameters and score for model\n",
    "    write_to_log_file(f\"Input dataframe: {pipeline['input_df']}, Preprocessing: {pipeline['preprocessing_steps']}\")\n",
    "    write_to_log_file(f\"Best Parameters for {models[model]['model']}: {grid_search.best_params_}\")\n",
    "    write_to_log_file(f\"Best Score for {models[model]['model']}: {grid_search.best_score_}\")\n",
    "    write_to_log_file(60*\"_\")\n",
    "\n",
    "start_time = time.time()\n",
    "for pipeline in pipelines:\n",
    "    print(\"NEW PIPELINE TEST:\")\n",
    "    print(pipeline['X'].shape)\n",
    "    models = hyperparameters['models']\n",
    "    for model in models:\n",
    "        data = models[model]\n",
    "        if 'preprocessing' in data:\n",
    "            for preprocessing in data['preprocessing']:\n",
    "                pipeline_data = []\n",
    "                pipeline_data.append(('preprocessing', eval(preprocessing)))\n",
    "                pipeline_data.append(('model', eval(models[model]['model'])))\n",
    "                pipe = Pipeline(pipeline_data)\n",
    "                grid_search(model, pipe, pipeline)\n",
    "\n",
    "        else:\n",
    "            pipeline_data = []\n",
    "            pipeline_data.append(('model', eval(models[model]['model'])))\n",
    "            pipe = Pipeline(pipeline_data)\n",
    "            grid_search(model, pipe, pipeline)\n",
    "\n",
    "write_to_log_file(f'{(time.time() - start_time)}')\n",
    "write_to_log_file(60*\"_\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pipeline in pipelines:\n",
    "    print(\"________________________-\")\n",
    "    print(f\"Input dataframe: {pipeline['input_df']}, Preprocessing: {pipeline['preprocessing_steps']}\")\n",
    "    print\n",
    "    baseline_regression_model(regression_models, pipeline['X_train'].drop(columns=['minerals', 'magnesium', 'flavanoids', 'calcium']), pipeline['X_test'].drop(columns=['minerals', 'magnesium', 'flavanoids', 'calcium']), pipeline['y_train'], pipeline['y_test'])\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best R² and MSE: RandomForestRegressor\n",
    "\n",
    "For the different pipelines we print the results and do an feature importance analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________\n",
      "Input dataframe: df_wine_clean, Preprocessing: univariate outlier + noise\n",
      "MSE: 0.38513485424588084\n",
      "R2: 0.652799369475306\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_clean, Preprocessing: multivariate outlier + noise\n",
      "MSE: 0.4154591315453385\n",
      "R2: 0.6607033028732799\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_clean, Preprocessing: uni-multivariate outlier + noise\n",
      "MSE: 0.402055938697318\n",
      "R2: 0.6785778526832984\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_no_gamay, Preprocessing: univariate outlier + noise\n",
      "MSE: 0.3996304177545692\n",
      "R2: 0.46369470706091165\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_no_gamay, Preprocessing: multivariate outlier + noise\n",
      "MSE: 0.4370177631578947\n",
      "R2: 0.4192187730083935\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_no_gamay, Preprocessing: uni-multivariate outlier + noise\n",
      "MSE: 0.37797447368421055\n",
      "R2: 0.4746400487054786\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_gamay, Preprocessing: univariate outlier + noise\n",
      "MSE: 0.17331304347826088\n",
      "R2: 0.29474923076923043\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_gamay, Preprocessing: multivariate outlier + noise\n",
      "MSE: 0.1746391304347826\n",
      "R2: 0.1751419642857145\n",
      "\n",
      "\n",
      "________________________\n",
      "Input dataframe: df_wine_gamay, Preprocessing: uni-multivariate outlier + noise\n",
      "MSE: 0.20631304347826085\n",
      "R2: 0.025539285714285986\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfc = RandomForestRegressor(random_state=42)\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    print(\"________________________\")\n",
    "    print(f\"Input dataframe: {pipeline['input_df']}, Preprocessing: {pipeline['preprocessing_steps']}\")\n",
    "\n",
    "\n",
    "    rfc.fit(pipeline['X_train'], pipeline['y_train'])\n",
    "\n",
    "    y_pred = rfc.predict(pipeline['X_test'])\n",
    "\n",
    "    # Calculate the mean squared error and R-squared\n",
    "    mse = mean_squared_error(pipeline['y_test'], y_pred)\n",
    "    r2 = r2_score(pipeline['y_test'], y_pred)\n",
    "    write_to_log_file(f\"MSE: {mse}\")\n",
    "    write_to_log_file(f\"R2: {r2}\")\n",
    "    feat_importances = pd.Series(rfc.feature_importances_, index = pipeline['X_train'].columns).sort_values(ascending = True)\n",
    "    feat_importances.plot(kind = 'barh')\n",
    "    plt.title('Feature Importances RandomForest')\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Clean dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "array([5.75, 5.37, 5.98, 4.93, 6.68, 5.63, 1.96, 5.7 , 5.85, 5.3 , 5.36,\n       6.16, 6.5 , 5.92, 5.27, 6.8 , 5.19, 6.6 , 5.73, 5.19, 6.55, 6.08,\n       6.14, 5.19, 5.3 , 5.61, 5.78, 5.95, 6.48, 5.27, 6.35, 6.62, 5.61,\n       5.33, 6.07, 6.27, 6.11, 6.08, 5.63, 5.49, 5.56, 6.52, 5.79, 5.98,\n       6.05, 6.31, 5.32, 6.37, 6.21, 6.1 , 5.92, 6.23, 5.94, 6.26, 5.36,\n       5.66, 6.07, 6.26, 6.88, 6.65, 5.26, 6.3 , 6.07, 5.77, 5.61, 6.07,\n       6.58, 6.63, 6.29, 5.95, 6.03, 5.22, 6.62, 5.22, 6.43, 6.01, 6.28,\n       5.76, 5.36, 5.91, 1.93, 6.38, 5.17, 5.75, 5.81, 5.95, 6.52, 5.35,\n       6.55, 6.15, 5.63, 5.46, 5.4 , 6.75, 5.81, 5.4 , 6.84, 5.09, 6.17,\n       6.54, 5.92, 5.67, 5.34, 6.64, 5.95, 6.17, 4.88, 5.42, 5.99, 6.94,\n       1.59, 5.75, 6.45, 1.5 , 6.35, 5.11, 6.17, 7.68, 6.04, 5.85, 5.42,\n       5.86, 6.67, 5.83, 6.92, 6.2 , 6.06, 5.51, 5.36, 6.34, 5.43, 5.57,\n       5.14, 5.24, 6.3 , 6.1 , 6.56, 4.75, 6.36, 6.06, 5.87, 5.24, 5.89,\n       5.58, 5.27, 5.79, 6.4 , 5.6 , 6.02, 5.49, 5.79, 5.81, 6.68, 5.32,\n       1.29, 6.74, 6.02, 5.52, 6.21, 5.52, 6.79, 5.53, 6.03, 5.09, 5.17,\n       6.12, 7.04, 4.44, 5.62, 5.75, 5.82, 5.1 , 6.49, 5.52, 5.75, 5.38,\n       6.3 , 5.87, 5.24, 5.35, 5.18, 5.45, 6.78, 5.48, 6.15, 6.13, 6.  ,\n       6.12, 6.  , 6.39, 6.66, 5.41, 1.83, 6.73, 5.78, 5.9 , 6.51, 6.17,\n       6.43, 5.32, 5.27, 5.53, 5.35, 5.94, 5.94, 5.15, 6.15, 7.19, 6.25,\n       5.86, 5.81, 6.07, 6.06, 6.27, 5.57, 6.09, 6.56, 6.71, 6.16, 4.7 ,\n       5.75, 6.84, 6.67, 5.86, 1.55, 6.59, 4.69, 5.85, 5.61, 5.99, 6.19,\n       6.03, 5.04, 5.88, 6.28, 4.65, 6.42, 6.13, 5.97, 1.35, 6.14, 1.62,\n       5.22, 5.69, 5.79, 6.58, 5.19, 5.58, 6.23, 5.67, 6.86, 5.9 , 1.93,\n       6.82, 5.38, 6.  , 5.48, 1.95, 6.57, 4.81, 6.55, 5.5 , 6.2 , 5.42,\n       5.76, 6.3 , 6.38, 7.56, 5.91, 5.13, 5.3 , 7.22, 5.77, 5.86, 5.47,\n       6.43, 6.52, 5.99, 7.2 , 5.68, 6.03, 5.16, 1.92, 6.19, 5.7 , 5.83,\n       6.06, 5.53, 5.46, 6.63, 6.68, 5.7 , 6.97, 5.62, 5.93, 5.26, 7.02,\n       6.36, 5.8 , 5.24, 6.43, 6.65, 6.37, 5.98, 5.85, 5.7 , 5.69, 5.65,\n       6.25, 5.09, 6.18, 6.55, 5.51, 5.78, 6.13, 5.48, 5.5 , 5.17, 6.06,\n       4.94, 6.59, 5.64, 6.21, 6.79, 1.56, 5.26, 6.47, 5.26, 6.17, 5.4 ,\n       6.13, 5.9 , 5.2 , 7.16, 6.01, 6.58, 6.06, 5.85, 6.07, 6.46, 4.92,\n       6.16, 6.28, 6.3 , 6.01, 5.16, 6.28, 6.  , 6.01, 5.59, 6.59, 6.  ,\n       1.24, 5.06, 6.51, 5.28, 5.89, 5.62, 5.32, 5.75, 5.46, 6.69, 5.9 ,\n       5.41, 6.46, 6.58, 5.59, 6.82, 5.48, 6.95, 5.17, 5.77, 5.71, 5.62,\n       5.79, 6.46, 5.89, 6.54, 5.56, 6.36, 6.02, 5.69, 6.08, 5.49, 5.26,\n       6.35, 5.72, 5.95, 6.65, 6.01, 7.51, 1.79, 5.46, 7.63, 6.83, 6.12,\n       5.28, 5.13, 5.17, 7.24, 5.3 , 7.01, 5.06, 5.23, 5.32, 5.46, 6.4 ,\n       6.14, 5.85, 6.37, 5.68, 5.05, 5.99, 5.77, 5.14, 5.16, 5.34, 5.79,\n       6.02, 6.72, 5.6 , 5.09, 5.27, 5.49, 5.74, 5.49, 5.35, 6.06, 6.43,\n       5.24, 5.6 , 5.42, 6.53, 5.83, 5.74, 5.69, 5.91, 6.94, 6.37, 5.88,\n       5.67, 6.37, 6.91, 5.35, 6.79, 6.05, 5.23, 5.83, 6.63, 5.73, 5.28,\n       5.72, 6.19, 5.21, 5.44, 6.15, 4.99, 5.53, 6.47, 5.23, 6.67, 5.06,\n       6.18, 6.06, 5.63, 6.31, 5.32, 6.75, 6.13, 6.52, 6.15, 6.43, 5.8 ,\n       5.61, 6.21, 1.19, 5.05, 4.57, 5.04, 5.78, 6.65, 6.2 , 5.14, 5.96,\n       6.6 , 5.28, 5.85, 5.76, 6.25, 5.72, 6.58, 1.87, 7.48, 5.34, 5.34,\n       6.26, 6.07, 5.42, 5.99, 5.75, 6.85, 6.02, 6.03, 5.48, 6.14, 5.16,\n       6.  , 5.23, 5.79, 5.63, 5.08, 5.38, 6.09, 5.44, 5.5 , 4.75, 6.12,\n       5.74, 6.8 , 6.13, 6.23, 6.18, 5.88, 5.84, 5.53, 5.39, 5.37, 5.95,\n       5.14, 6.62, 5.77, 5.23, 5.33, 5.09, 5.75, 6.7 , 5.86, 5.74, 6.86,\n       6.08, 6.37, 5.9 , 5.62, 6.18, 5.89, 5.13, 6.44, 6.76, 5.32, 6.17,\n       5.95, 6.64, 7.63, 5.56, 5.93, 6.87, 5.35, 6.92, 5.28, 1.62, 5.53,\n       5.7 , 5.32, 5.43, 5.52, 6.05, 5.16, 5.08, 5.39, 5.19, 5.15, 6.19,\n       5.51, 6.06, 6.31, 5.6 , 6.58, 5.06, 5.98, 5.99, 6.28, 5.85, 5.69,\n       6.71, 5.32, 6.41, 5.39, 5.63, 6.19, 6.39, 5.69, 5.76, 6.13, 5.58,\n       7.01, 5.29, 6.57, 6.73, 5.44, 5.28, 5.6 , 5.58, 6.55, 5.68, 1.24,\n       6.34, 6.24, 5.97, 6.09, 5.83, 5.96, 6.16, 6.05, 5.87, 6.91, 5.17,\n       6.15, 5.15, 5.52, 6.71, 6.53, 1.96, 6.58, 5.68, 6.65, 5.77, 5.06,\n       5.94, 6.09, 6.43, 5.63, 5.3 , 5.79, 6.05, 6.65, 6.07, 5.34, 5.5 ,\n       5.34, 6.8 , 5.62, 5.98, 5.91, 6.22, 5.78, 5.8 , 6.35, 6.24, 5.15,\n       6.18, 6.47, 6.47, 6.28, 5.11, 5.3 , 5.41, 5.25, 5.22, 5.14, 5.73,\n       5.08, 6.09, 6.11, 5.21, 5.93, 5.25, 5.2 , 6.03, 5.43, 5.98, 5.81,\n       6.09, 5.33, 5.84, 7.15, 5.77, 4.79, 6.71, 5.72, 5.87, 5.48, 5.98,\n       5.84, 5.08, 6.14, 6.08, 5.74, 5.65, 5.86, 5.15, 5.54, 5.66, 6.68,\n       6.92, 6.63, 5.45, 1.2 , 5.39, 6.84, 6.24, 6.05, 6.05, 6.68, 5.35,\n       5.52, 5.94, 6.79, 6.74, 6.02, 6.16, 5.84, 7.69, 6.49, 6.54, 6.21,\n       5.3 , 6.07, 5.83, 5.55, 5.25, 6.5 , 5.83, 5.5 , 5.2 , 6.2 , 5.26,\n       5.07, 6.06, 6.15, 6.19, 5.61, 5.72, 5.66, 5.78, 1.4 , 1.95, 5.35,\n       4.88, 5.3 , 5.91, 6.28, 5.4 , 6.85, 6.37, 5.91, 6.08, 5.46, 6.21,\n       6.15, 6.62, 6.32, 5.62, 1.84, 5.64, 5.83, 6.64, 5.6 , 5.2 , 5.93,\n       6.1 , 5.13, 5.47, 5.27, 5.27, 5.85, 5.17, 5.24, 4.93, 6.19, 6.51,\n       6.16, 6.05, 5.1 , 5.31, 5.43, 5.48, 6.13, 5.36, 4.99, 5.37, 6.31,\n       6.37, 5.24])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestRegressor(random_state = 42).fit(pipeline3_clean['X_train'], pipeline3_clean['y_train'])\n",
    "y_pred_rf = model_rf.predict(pipeline3_clean['X_test'])\n",
    "y_pred_rf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(random_state=42)\n",
    "\n",
    "n_estimators = [1600]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "params_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=2, estimator=RandomForestRegressor(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'max_depth': [10, 20, 30, 40, 50, 60,\n                                                      70, 80, 90, 100, 110,\n                                                      None],\n                                        'max_features': ['auto', 'sqrt',\n                                                         'log2'],\n                                        'min_samples_leaf': [1, 2, 4],\n                                        'min_samples_split': [2, 5, 10],\n                                        'n_estimators': [1600]},\n                   random_state=42, verbose=1)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(estimator = rfc,  param_distributions = params_grid, verbose = 1, cv = 2, n_iter = 100, random_state = 42, n_jobs = -1)\n",
    "\n",
    "random_search.fit(pipeline3_clean['X_train'], pipeline3_clean['y_train'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: {'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 30, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "write_to_log_file(f'best: {random_search.best_params_}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "rf_rs = RandomForestRegressor(**random_search.best_params_)\n",
    "# n_estimators = 1600, min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 80, bootstrap = False, random_state = 42\n",
    "\n",
    "model_rf_rs = rf_rs.fit(pipeline3_clean['X_train'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']), pipeline3_clean['y_train'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']))\n",
    "\n",
    "y_pred_rf_rs = model_rf_rs.predict(pipeline3_clean['X_test'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score MAE for  RF:        0.4843\n",
      "Score MAPE for RF:        0.0906\n",
      "Score MSE for  RF:        0.4021\n",
      "Score RMSE for RF:        0.6341\n",
      "------------------------------------------------------------\n",
      "Score MAE for  Tuned RF:  0.4477\n",
      "Score MAPE for Tuned RF:  0.0840\n",
      "Score MSE for  Tuned RF:  0.3648\n",
      "Score RMSE for Tuned RF:  0.6040\n"
     ]
    }
   ],
   "source": [
    "y_test = pipeline3_clean['y_test']\n",
    "write_to_log_file('Score MAE for  RF:        {:.4f}' .format(mean_absolute_error(y_test, y_pred_rf)))\n",
    "write_to_log_file('Score MAPE for RF:        {:.4f}' .format(mean_absolute_percentage_error(y_test, y_pred_rf)))\n",
    "write_to_log_file('Score MSE for  RF:        {:.4f}' .format(mean_squared_error(y_test, y_pred_rf, squared = True)))\n",
    "write_to_log_file('Score RMSE for RF:        {:.4f}' .format(mean_squared_error(y_test, y_pred_rf, squared = False)))\n",
    "write_to_log_file(60*'-')\n",
    "write_to_log_file('Score MAE for  Tuned RF:  {:.4f}' .format(mean_absolute_error(y_test, y_pred_rf_rs)))\n",
    "write_to_log_file('Score MAPE for Tuned RF:  {:.4f}' .format(mean_absolute_percentage_error(y_test, y_pred_rf_rs)))\n",
    "write_to_log_file('Score MSE for  Tuned RF:  {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_rs, squared = True)))\n",
    "write_to_log_file('Score RMSE for Tuned RF:  {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_rs, squared = False)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "      Cabernet Sauvignon  Chardonnay  Gamay  Merlot  Pinot noir  real quality  \\\n1821                 0.0         0.0    0.0     0.0         1.0             7   \n3270                 0.0         0.0    0.0     0.0         1.0             7   \n165                  1.0         0.0    0.0     0.0         0.0             6   \n1333                 0.0         0.0    0.0     1.0         0.0             6   \n3759                 0.0         0.0    0.0     1.0         0.0             6   \n413                  0.0         1.0    0.0     0.0         0.0             5   \n3058                 1.0         0.0    0.0     0.0         0.0             5   \n630                  0.0         0.0    0.0     0.0         1.0             6   \n3228                 0.0         0.0    0.0     1.0         0.0             5   \n2148                 1.0         0.0    0.0     0.0         0.0             5   \n2818                 1.0         0.0    0.0     0.0         0.0             5   \n171                  0.0         1.0    0.0     0.0         0.0             6   \n3954                 1.0         0.0    0.0     0.0         0.0             6   \n2325                 1.0         0.0    0.0     0.0         0.0             8   \n2185                 0.0         0.0    0.0     0.0         1.0             5   \n\n      rf pred  tuned rf pred  \n1821     6.19       6.128125  \n3270     6.51       6.644375  \n165      6.16       6.110625  \n1333     6.05       6.065000  \n3759     5.10       5.093125  \n413      5.31       5.347500  \n3058     5.43       5.330000  \n630      5.48       5.578750  \n3228     6.13       6.005000  \n2148     5.36       5.246250  \n2818     4.99       5.076250  \n171      5.37       5.296250  \n3954     6.31       6.270000  \n2325     6.37       6.225625  \n2185     5.24       5.112517  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cabernet Sauvignon</th>\n      <th>Chardonnay</th>\n      <th>Gamay</th>\n      <th>Merlot</th>\n      <th>Pinot noir</th>\n      <th>real quality</th>\n      <th>rf pred</th>\n      <th>tuned rf pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1821</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>6.19</td>\n      <td>6.128125</td>\n    </tr>\n    <tr>\n      <th>3270</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>6.51</td>\n      <td>6.644375</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>6.16</td>\n      <td>6.110625</td>\n    </tr>\n    <tr>\n      <th>1333</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>6.05</td>\n      <td>6.065000</td>\n    </tr>\n    <tr>\n      <th>3759</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>5.10</td>\n      <td>5.093125</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>5.31</td>\n      <td>5.347500</td>\n    </tr>\n    <tr>\n      <th>3058</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>5.43</td>\n      <td>5.330000</td>\n    </tr>\n    <tr>\n      <th>630</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>5.48</td>\n      <td>5.578750</td>\n    </tr>\n    <tr>\n      <th>3228</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>6.13</td>\n      <td>6.005000</td>\n    </tr>\n    <tr>\n      <th>2148</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>5.36</td>\n      <td>5.246250</td>\n    </tr>\n    <tr>\n      <th>2818</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>4.99</td>\n      <td>5.076250</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>5.37</td>\n      <td>5.296250</td>\n    </tr>\n    <tr>\n      <th>3954</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>6.31</td>\n      <td>6.270000</td>\n    </tr>\n    <tr>\n      <th>2325</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>6.37</td>\n      <td>6.225625</td>\n    </tr>\n    <tr>\n      <th>2185</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.24</td>\n      <td>5.112517</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_display = pipeline3_clean['X_test'].copy()\n",
    "df_display = df_display.drop(['fixed_acidity', 'volatile_acidity', 'citric_acid', 'magnesium','flavanoids', 'minerals', 'calcium', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol'], axis= 1)\n",
    "df_display['real quality'] = y_test\n",
    "df_display['rf pred'] = y_pred_rf\n",
    "df_display['tuned rf pred'] = y_pred_rf_rs\n",
    "df_display.tail(15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test No Gamay dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "array([6.03, 5.85, 6.71, 5.33, 6.77, 5.92, 5.79, 5.38, 5.16, 5.5 , 5.8 ,\n       5.31, 5.6 , 5.91, 6.71, 6.42, 5.34, 5.25, 6.4 , 6.  , 4.8 , 6.27,\n       6.  , 5.05, 5.76, 5.35, 6.41, 5.58, 5.32, 5.5 , 6.32, 5.54, 5.48,\n       5.84, 5.71, 6.  , 6.69, 5.22, 6.55, 6.25, 5.72, 6.81, 5.88, 6.59,\n       6.1 , 6.76, 6.58, 5.63, 5.33, 5.35, 5.97, 5.23, 5.89, 5.12, 6.23,\n       5.76, 5.34, 6.09, 5.27, 6.38, 4.93, 6.07, 6.62, 6.28, 6.23, 5.64,\n       5.71, 6.58, 5.89, 6.19, 6.  , 6.33, 5.89, 5.39, 4.94, 4.89, 5.28,\n       5.44, 5.48, 6.39, 5.72, 5.27, 5.39, 6.18, 5.83, 5.86, 5.65, 6.72,\n       5.74, 6.14, 6.99, 6.11, 6.51, 5.36, 5.45, 6.15, 5.32, 5.85, 6.15,\n       6.79, 5.63, 5.16, 6.55, 4.54, 5.34, 5.68, 6.27, 5.47, 6.57, 5.79,\n       5.71, 4.93, 5.32, 6.55, 6.11, 5.34, 6.4 , 5.36, 5.82, 6.  , 5.42,\n       5.93, 5.11, 6.16, 5.79, 6.67, 6.5 , 5.68, 5.65, 5.5 , 5.53, 6.74,\n       6.54, 7.02, 5.81, 5.86, 5.22, 5.17, 5.95, 5.43, 5.87, 6.07, 6.45,\n       5.79, 5.22, 5.32, 5.82, 6.48, 5.41, 6.01, 6.34, 5.26, 6.9 , 6.85,\n       6.43, 6.96, 5.29, 7.02, 6.76, 6.79, 5.47, 5.41, 5.55, 6.19, 5.15,\n       6.17, 5.53, 6.25, 6.19, 5.55, 6.19, 6.06, 6.09, 6.01, 6.37, 6.4 ,\n       5.68, 5.57, 5.32, 6.19, 6.28, 6.51, 6.66, 6.01, 5.54, 5.07, 6.33,\n       5.28, 6.36, 5.57, 6.72, 6.56, 7.  , 6.23, 5.84, 6.18, 5.91, 5.39,\n       6.45, 5.47, 5.04, 5.19, 6.  , 6.45, 5.28, 5.81, 6.16, 6.08, 6.51,\n       6.58, 6.32, 5.75, 6.39, 6.5 , 5.06, 5.53, 4.92, 5.72, 5.74, 5.24,\n       6.04, 7.02, 6.33, 5.54, 4.95, 5.68, 5.12, 5.26, 5.28, 5.19, 5.1 ,\n       5.98, 5.88, 5.36, 5.35, 6.61, 5.28, 5.46, 6.  , 5.94, 6.35, 5.81,\n       6.08, 6.21, 6.27, 6.07, 7.13, 6.57, 5.43, 5.67, 6.54, 5.32, 6.12,\n       5.68, 5.85, 4.46, 5.36, 6.53, 6.28, 5.82, 7.15, 5.81, 6.18, 5.23,\n       5.89, 5.28, 7.21, 5.38, 5.75, 5.56, 5.44, 6.33, 5.11, 6.34, 6.84,\n       5.48, 6.44, 6.04, 7.24, 5.83, 6.38, 5.68, 4.65, 6.77, 5.23, 6.19,\n       6.07, 5.56, 5.92, 4.76, 6.18, 6.13, 5.68, 5.98, 5.89, 5.57, 5.42,\n       6.99, 6.11, 7.45, 5.83, 5.46, 5.6 , 6.6 , 4.5 , 6.52, 6.74, 6.03,\n       5.37, 6.44, 5.3 , 5.5 , 5.79, 5.25, 5.26, 5.72, 6.18, 5.61, 4.96,\n       5.64, 5.53, 5.65, 5.68, 6.83, 6.99, 6.06, 4.86, 5.12, 5.94, 5.27,\n       6.03, 5.89, 5.43, 6.1 , 6.41, 5.22, 6.61, 5.16, 5.29, 6.58, 5.91,\n       5.37, 6.14, 5.68, 5.32, 6.59, 6.82, 5.98, 6.32, 5.69, 5.17, 5.11,\n       4.92, 6.88, 5.1 , 5.56, 5.75, 6.05, 6.67, 5.47, 5.58, 5.7 , 5.5 ,\n       5.35, 5.94, 5.62, 7.01, 5.46, 5.28, 5.16, 6.  , 5.79, 5.32, 5.91,\n       7.22, 5.21, 5.18, 7.12, 6.3 , 6.73, 6.48, 5.06, 6.37, 6.19, 5.8 ,\n       6.57, 4.77, 5.15, 5.76, 5.17, 6.63, 5.31, 5.08, 5.1 , 5.88, 5.48,\n       5.78, 5.25, 5.13, 5.86, 5.42, 6.81, 6.35, 5.32, 5.63, 4.89, 6.94,\n       5.26, 6.72, 4.8 , 6.36, 5.52, 6.3 , 6.46, 5.5 , 5.08, 6.34, 6.29,\n       5.25, 4.89, 6.49, 6.12, 5.92, 6.28, 5.34, 5.29, 6.08, 6.27, 5.96,\n       6.15, 6.91, 5.29, 4.71, 5.65, 5.3 , 5.85, 6.82, 6.76, 5.75, 6.67,\n       6.05, 5.26, 6.58, 6.96, 5.35, 5.28, 6.53, 5.58, 6.74, 6.58, 5.79,\n       5.35, 6.05, 6.58, 6.8 , 5.25, 5.67, 6.35, 5.92, 4.99, 5.41, 6.02,\n       6.55, 6.08, 6.65, 5.94, 5.71, 5.7 , 5.52, 6.89, 6.92, 5.46, 5.47,\n       5.07, 6.02, 5.87, 5.44, 6.66, 6.64, 5.13, 4.82, 6.44, 5.15, 4.91,\n       6.65, 5.16, 5.19, 6.63, 6.23, 5.8 , 6.3 , 6.07, 6.03, 6.32, 5.62,\n       6.13, 5.56, 5.85, 6.  , 7.01, 6.02, 5.84, 6.21, 6.52, 6.74, 6.02,\n       5.72, 5.44, 6.4 , 5.09, 5.14, 6.08, 5.18, 5.83, 6.58, 6.46, 6.62,\n       6.88, 6.44, 5.96, 6.27, 6.63, 6.66, 6.28, 5.8 , 5.74, 6.41, 5.65,\n       5.31, 5.9 , 6.41, 5.17, 5.22, 5.62, 5.98, 6.39, 5.8 , 5.16, 5.62,\n       6.1 , 5.55, 5.72, 5.52, 5.34, 5.39, 6.46, 5.9 , 6.67, 5.88, 5.91,\n       6.36, 6.2 , 5.48, 6.55, 6.34, 5.2 , 6.78, 5.92, 6.02, 6.68, 7.21,\n       6.93, 6.3 , 5.96, 5.9 , 6.73, 6.64, 6.18, 5.42, 5.7 , 6.35, 6.02,\n       6.43, 5.39, 5.07, 5.13, 6.04, 5.17, 5.97, 6.32, 6.65, 6.67, 5.12,\n       5.84, 5.89, 5.73, 6.09, 5.85, 5.54, 5.14, 5.54, 5.25, 5.78, 5.88,\n       6.4 , 5.36, 5.61, 6.24, 5.55, 5.48, 6.07, 5.26, 5.31, 5.72, 6.32,\n       5.45, 6.06, 6.99, 5.72, 6.28, 6.61, 6.59, 5.27, 5.76, 5.9 , 5.04,\n       6.64, 5.83, 5.81, 5.59, 6.12, 5.94, 5.07, 6.64, 5.5 , 5.64, 6.49,\n       6.02, 5.15, 5.73, 5.78, 5.49, 5.31, 5.51, 6.93, 6.62, 5.62, 5.98,\n       4.56, 5.84, 6.64, 6.43, 5.59, 6.07, 5.98, 5.18, 5.67, 5.93, 6.08,\n       5.62, 5.1 , 5.46, 5.93, 5.15, 5.1 , 5.41, 6.44, 5.86, 6.71, 5.18,\n       5.02, 5.12, 5.67, 6.01, 6.49, 4.74, 5.65, 5.1 , 5.92, 5.72, 5.27,\n       5.24, 6.24, 6.25, 5.11, 6.43, 6.99, 6.15, 5.91, 5.55, 5.28, 6.49,\n       4.84, 6.05, 6.51, 5.93, 6.65, 4.97, 5.91, 6.61, 4.95, 5.21, 6.34,\n       6.87, 6.52, 5.74, 5.3 , 5.4 , 6.  , 5.85, 6.  , 5.53, 5.23, 6.63,\n       5.97, 5.56, 6.32, 5.6 , 4.95, 5.44, 4.76, 6.21, 6.85, 6.74, 4.99,\n       5.84, 5.52, 5.46, 6.53, 5.48, 6.53, 5.93, 6.38, 6.26, 6.46, 6.17,\n       6.35, 5.91, 6.31, 4.91, 5.23, 5.93, 5.61, 6.42, 5.29, 5.02, 4.94,\n       7.04, 4.49, 5.48, 6.98, 5.16, 5.34, 5.35, 5.16, 5.84, 5.97, 7.02,\n       5.96, 5.79, 6.02, 6.98, 4.79, 6.94, 6.92, 5.46, 5.69, 5.48, 5.5 ,\n       5.72])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf_ngm = RandomForestRegressor(random_state = 42).fit(pipeline3_no_gamay['X_train'], pipeline3_no_gamay['y_train'])\n",
    "y_pred_rf_ngm = model_rf_ngm.predict(pipeline3_no_gamay['X_test'])\n",
    "y_pred_rf_ngm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(random_state=42)\n",
    "\n",
    "n_estimators = [1400, 1600, 1800]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "params_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'max_depth': [10, 20, 30, 40, 50, 60,\n                                                      70, 80, 90, 100, 110,\n                                                      None],\n                                        'max_features': ['auto', 'sqrt',\n                                                         'log2'],\n                                        'min_samples_leaf': [1, 2, 4],\n                                        'min_samples_split': [2, 5, 10],\n                                        'n_estimators': [1400, 1600, 1800]},\n                   random_state=42, verbose=1)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(estimator = rfc,  param_distributions = params_grid, verbose = 1, cv = 5, n_iter = 100, random_state = 42, n_jobs = -1)\n",
    "\n",
    "random_search.fit(pipeline3_no_gamay['X_train'], pipeline3_no_gamay['y_train'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final model No Gamay\n",
    "\n",
    "Dropping 'magnesium', 'flavanoids', 'minerals', 'calcium' to further improve model based on Task 1 Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: {'n_estimators': 1400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 70, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "write_to_log_file(f'best: {random_search.best_params_}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "rf_rs = RandomForestRegressor(**random_search.best_params_)\n",
    "# n_estimators = 1600, min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 80, bootstrap = False, random_state = 42\n",
    "\n",
    "model_rf_rs = rf_rs.fit(pipeline3_no_gamay['X_train'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']), pipeline3_no_gamay['y_train'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']))\n",
    "\n",
    "# save model\n",
    "filename_ng = 'finalized_model_no_gamay.sav'\n",
    "joblib.dump(model, filename_ng)\n",
    "\n",
    "y_pred_rf_ngm_rs = model_rf_rs.predict(pipeline3_no_gamay['X_test'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score MAE for  RF:        0.4742\n",
      "Score MAPE for RF:        0.0836\n",
      "Score MSE for  RF:        0.3780\n",
      "Score RMSE for RF:        0.6148\n",
      "------------------------------------------------------------\n",
      "Score MAE for  Tuned RF:  0.3964\n",
      "Score MAPE for Tuned RF:  0.0701\n",
      "Score MSE for  Tuned RF:  0.3155\n",
      "Score RMSE for Tuned RF:  0.5617\n"
     ]
    }
   ],
   "source": [
    "y_test = pipeline3_no_gamay['y_test']\n",
    "write_to_log_file('Score MAE for  RF:        {:.4f}' .format(mean_absolute_error(y_test, y_pred_rf_ngm)))\n",
    "write_to_log_file('Score MAPE for RF:        {:.4f}' .format(mean_absolute_percentage_error(y_test, y_pred_rf_ngm)))\n",
    "write_to_log_file('Score MSE for  RF:        {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_ngm, squared = True)))\n",
    "write_to_log_file('Score RMSE for RF:        {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_ngm, squared = False)))\n",
    "write_to_log_file(60*'-')\n",
    "write_to_log_file('Score MAE for  Tuned RF:  {:.4f}' .format(mean_absolute_error(y_test, y_pred_rf_ngm_rs)))\n",
    "write_to_log_file('Score MAPE for Tuned RF:  {:.4f}' .format(mean_absolute_percentage_error(y_test, y_pred_rf_ngm_rs)))\n",
    "write_to_log_file('Score MSE for  Tuned RF:  {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_ngm_rs, squared = True)))\n",
    "write_to_log_file('Score RMSE for Tuned RF:  {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_ngm_rs, squared = False)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "      Cabernet Sauvignon  Chardonnay  Merlot  Pinot noir  real quality  \\\n3004                 1.0         0.0     0.0         0.0             5   \n3158                 0.0         0.0     0.0         1.0             5   \n2573                 0.0         0.0     0.0         1.0             7   \n33                   0.0         0.0     0.0         1.0             5   \n861                  0.0         0.0     0.0         1.0             6   \n2849                 0.0         1.0     0.0         0.0             6   \n1740                 0.0         0.0     0.0         1.0             7   \n609                  0.0         0.0     0.0         1.0             4   \n3117                 0.0         1.0     0.0         0.0             7   \n3824                 1.0         0.0     0.0         0.0             7   \n2216                 1.0         0.0     0.0         0.0             6   \n631                  0.0         0.0     0.0         1.0             5   \n1127                 0.0         1.0     0.0         0.0             6   \n1983                 0.0         0.0     1.0         0.0             7   \n1338                 0.0         0.0     1.0         0.0             6   \n\n      rf pred  tuned rf pred  \n3004     5.84       5.947143  \n3158     5.97       6.054286  \n2573     7.02       6.984286  \n33       5.96       5.881429  \n861      5.79       5.645714  \n2849     6.02       5.847143  \n1740     6.98       6.954286  \n609      4.79       4.297857  \n3117     6.94       6.715000  \n3824     6.92       7.011429  \n2216     5.46       5.897857  \n631      5.69       5.552857  \n1127     5.48       5.519286  \n1983     5.50       5.369286  \n1338     5.72       5.840714  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cabernet Sauvignon</th>\n      <th>Chardonnay</th>\n      <th>Merlot</th>\n      <th>Pinot noir</th>\n      <th>real quality</th>\n      <th>rf pred</th>\n      <th>tuned rf pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3004</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>5.84</td>\n      <td>5.947143</td>\n    </tr>\n    <tr>\n      <th>3158</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.97</td>\n      <td>6.054286</td>\n    </tr>\n    <tr>\n      <th>2573</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>7.02</td>\n      <td>6.984286</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.96</td>\n      <td>5.881429</td>\n    </tr>\n    <tr>\n      <th>861</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>5.79</td>\n      <td>5.645714</td>\n    </tr>\n    <tr>\n      <th>2849</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>6.02</td>\n      <td>5.847143</td>\n    </tr>\n    <tr>\n      <th>1740</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>6.98</td>\n      <td>6.954286</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>4.79</td>\n      <td>4.297857</td>\n    </tr>\n    <tr>\n      <th>3117</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>6.94</td>\n      <td>6.715000</td>\n    </tr>\n    <tr>\n      <th>3824</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>6.92</td>\n      <td>7.011429</td>\n    </tr>\n    <tr>\n      <th>2216</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>5.46</td>\n      <td>5.897857</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.69</td>\n      <td>5.552857</td>\n    </tr>\n    <tr>\n      <th>1127</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>5.48</td>\n      <td>5.519286</td>\n    </tr>\n    <tr>\n      <th>1983</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>5.50</td>\n      <td>5.369286</td>\n    </tr>\n    <tr>\n      <th>1338</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>5.72</td>\n      <td>5.840714</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_display = pipeline3_no_gamay['X_test'].copy()\n",
    "df_display = df_display.drop(['fixed_acidity', 'volatile_acidity', 'citric_acid', 'magnesium','flavanoids', 'minerals', 'calcium', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol'], axis= 1)\n",
    "df_display['real quality'] = y_test\n",
    "df_display['rf pred'] = y_pred_rf_ngm\n",
    "df_display['tuned rf pred'] = y_pred_rf_ngm_rs\n",
    "df_display.tail(15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gamay\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.91, 1.67, 1.99, 1.92, 1.37, 1.94, 1.42, 1.91, 1.89, 1.96, 1.88,\n       1.71, 2.  , 1.32, 1.58, 1.66, 1.8 , 1.66, 1.62, 1.73, 1.66, 1.7 ,\n       1.54])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf_gm = RandomForestRegressor(random_state = 42).fit(pipeline3_gamay['X_train'], pipeline3_gamay['y_train'])\n",
    "y_pred_rf_gm = model_rf_gm.predict(pipeline3_gamay['X_test'])\n",
    "y_pred_rf_gm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(random_state=42)\n",
    "\n",
    "n_estimators = [1400, 1600, 1800]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "params_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'max_depth': [10, 20, 30, 40, 50, 60,\n                                                      70, 80, 90, 100, 110,\n                                                      None],\n                                        'max_features': ['auto', 'sqrt',\n                                                         'log2'],\n                                        'min_samples_leaf': [1, 2, 4],\n                                        'min_samples_split': [2, 5, 10],\n                                        'n_estimators': [1400, 1600, 1800]},\n                   random_state=42, verbose=1)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_gamay = RandomizedSearchCV(estimator = rfc,  param_distributions = params_grid, verbose = 1, cv = 5, n_iter = 100, random_state = 42, n_jobs = -1)\n",
    "\n",
    "random_search_gamay.fit(pipeline3_gamay['X_train'], pipeline3_gamay['y_train'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Gamay model\n",
    "\n",
    "Dropping 'magnesium', 'flavanoids', 'minerals', 'calcium' to further improve model based on Task 1 Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: {'n_estimators': 1800, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "write_to_log_file(f'best: {random_search_gamay.best_params_}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "rf_rs_gamay = RandomForestRegressor(**random_search_gamay.best_params_)\n",
    "# n_estimators = 1600, min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 80, bootstrap = False, random_state = 42\n",
    "\n",
    "model_rf_rs_gm = rf_rs_gamay.fit(pipeline3_gamay['X_train'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']), pipeline3_gamay['y_train'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']))\n",
    "\n",
    "# save model\n",
    "filename_g = 'finalized_model_gamay.sav'\n",
    "joblib.dump(model, filename_g)\n",
    "\n",
    "y_pred_rf_gm_rs = model_rf_rs_gm.predict(pipeline3_gamay['X_test'].drop(columns=[ 'magnesium','flavanoids', 'minerals', 'calcium']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score MAE for  RF:        0.3643\n",
      "Score MAPE for RF:        0.2824\n",
      "Score MSE for  RF:        0.2063\n",
      "Score RMSE for RF:        0.4542\n",
      "------------------------------------------------------------\n",
      "Score MAE for  Tuned RF:  0.3894\n",
      "Score MAPE for Tuned RF:  0.2993\n",
      "Score MSE for  Tuned RF:  0.2115\n",
      "Score RMSE for Tuned RF:  0.4599\n"
     ]
    }
   ],
   "source": [
    "y_test = pipeline3_gamay['y_test']\n",
    "write_to_log_file('Score MAE for  RF:        {:.4f}' .format(mean_absolute_error(y_test, y_pred_rf_gm)))\n",
    "write_to_log_file('Score MAPE for RF:        {:.4f}' .format(mean_absolute_percentage_error(y_test, y_pred_rf_gm)))\n",
    "write_to_log_file('Score MSE for  RF:        {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_gm, squared = True)))\n",
    "write_to_log_file('Score RMSE for RF:        {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_gm, squared = False)))\n",
    "write_to_log_file(60*'-')\n",
    "write_to_log_file('Score MAE for  Tuned RF:  {:.4f}' .format(mean_absolute_error(y_test, y_pred_rf_gm_rs)))\n",
    "write_to_log_file('Score MAPE for Tuned RF:  {:.4f}' .format(mean_absolute_percentage_error(y_test, y_pred_rf_gm_rs)))\n",
    "write_to_log_file('Score MSE for  Tuned RF:  {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_gm_rs, squared = True)))\n",
    "write_to_log_file('Score RMSE for Tuned RF:  {:.4f}' .format(mean_squared_error(y_test, y_pred_rf_gm_rs, squared = False)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "      real quality  rf pred  tuned rf pred\n1236             2     1.89       1.952778\n3478             2     1.96       1.805185\n1829             2     1.88       1.774444\n446              2     1.71       1.691932\n1906             2     2.00       1.909921\n1819             2     1.32       1.495300\n2559             1     1.58       1.578704\n3594             2     1.66       1.602222\n108              1     1.80       1.761810\n1192             1     1.66       1.816667\n2534             2     1.62       1.556111\n436              2     1.73       1.581319\n1797             2     1.66       1.766667\n1272             1     1.70       1.713426\n2430             1     1.54       1.484444",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>real quality</th>\n      <th>rf pred</th>\n      <th>tuned rf pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1236</th>\n      <td>2</td>\n      <td>1.89</td>\n      <td>1.952778</td>\n    </tr>\n    <tr>\n      <th>3478</th>\n      <td>2</td>\n      <td>1.96</td>\n      <td>1.805185</td>\n    </tr>\n    <tr>\n      <th>1829</th>\n      <td>2</td>\n      <td>1.88</td>\n      <td>1.774444</td>\n    </tr>\n    <tr>\n      <th>446</th>\n      <td>2</td>\n      <td>1.71</td>\n      <td>1.691932</td>\n    </tr>\n    <tr>\n      <th>1906</th>\n      <td>2</td>\n      <td>2.00</td>\n      <td>1.909921</td>\n    </tr>\n    <tr>\n      <th>1819</th>\n      <td>2</td>\n      <td>1.32</td>\n      <td>1.495300</td>\n    </tr>\n    <tr>\n      <th>2559</th>\n      <td>1</td>\n      <td>1.58</td>\n      <td>1.578704</td>\n    </tr>\n    <tr>\n      <th>3594</th>\n      <td>2</td>\n      <td>1.66</td>\n      <td>1.602222</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>1</td>\n      <td>1.80</td>\n      <td>1.761810</td>\n    </tr>\n    <tr>\n      <th>1192</th>\n      <td>1</td>\n      <td>1.66</td>\n      <td>1.816667</td>\n    </tr>\n    <tr>\n      <th>2534</th>\n      <td>2</td>\n      <td>1.62</td>\n      <td>1.556111</td>\n    </tr>\n    <tr>\n      <th>436</th>\n      <td>2</td>\n      <td>1.73</td>\n      <td>1.581319</td>\n    </tr>\n    <tr>\n      <th>1797</th>\n      <td>2</td>\n      <td>1.66</td>\n      <td>1.766667</td>\n    </tr>\n    <tr>\n      <th>1272</th>\n      <td>1</td>\n      <td>1.70</td>\n      <td>1.713426</td>\n    </tr>\n    <tr>\n      <th>2430</th>\n      <td>1</td>\n      <td>1.54</td>\n      <td>1.484444</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_display_gamay = pipeline3_gamay['X_test'].copy()\n",
    "df_display_gamay = df_display_gamay.drop(['fixed_acidity', 'volatile_acidity', 'citric_acid', 'magnesium','flavanoids', 'minerals', 'calcium', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol'], axis= 1)\n",
    "df_display_gamay['real quality'] = y_test\n",
    "df_display_gamay['rf pred'] = y_pred_rf_gm\n",
    "df_display_gamay['tuned rf pred'] = y_pred_rf_gm_rs\n",
    "df_display_gamay.tail(15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "Our final models are saved in\n",
    "- 'finalized_model_gamay.sav': Score MSE for RandomforestRegressor:  0.2115\n",
    "- 'finalized_model_no_gamay.sav': Score MSE for RandomforestRegressor:  0.3155\n",
    "\n",
    "In comparison to our baseline model: MSE between roundabout 122 and 264"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
